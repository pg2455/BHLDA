@inproceedings{model_annotated_data,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '03},
	title = {Modeling Annotated Data},
	isbn = {1-58113-646-3},
	url = {http://doi.acm.org/10.1145/860435.860460},
	doi = {10.1145/860435.860460},
	abstract = {We consider the problem of modeling annotated data---data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image). We describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in correspondence latent Dirichlet allocation, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type. We conduct experiments on the Corel database of images and captions, assessing performance in terms of held-out likelihood, automatic annotation, and text-based image retrieval.},
	urldate = {2014-12-09},
	booktitle = {Proceedings of the 26th Annual International {ACM} {SIGIR} Conference on Research and Development in Informaion Retrieval},
	publisher = {{ACM}},
	author = {Blei, David M. and Jordan, Michael I.},
	year = {2003},
	keywords = {automatic image annotation, empirical Bayes, image retrieval, probabilistic graphical models, variational methods},
	pages = {127--134}
}


@misc{wiki,
	title = {Latent Dirichlet allocation},
	copyright = {Creative Commons Attribution-{ShareAlike} License},
	url = {http://en.wikipedia.org/w/index.php?title=Latent_Dirichlet_allocation&oldid=633438913},
	abstract = {In natural language processing, latent Dirichlet allocation ({LDA}) is a generative model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. {LDA} is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael Jordan in 2003.[1]},
	language = {en},
	urldate = {2014-12-14},
	journal = {Wikipedia, the free encyclopedia},
	month = dec,
	year = {2014},
	note = {Page Version {ID}: 633438913},
	file = {Snapshot:/Users/mac/Library/Application Support/Firefox/Profiles/u7ckyl4g.default/zotero/storage/T24AVEG4/index.html:text/html}
}

@misc{corpus,
	title = {The {N}ew {Y}ork {T}imes {A}nnotated {C}orpus - {L}inguistic {D}ata {C}onsortium {LDC}2008{T}19. {W}eb {D}ownload. {P}hiladelphia: {L}inguistic {D}ata {C}onsortium, 2008 },
	url = {https://catalog.ldc.upenn.edu/LDC2008T19},
	urldate = {2014-12-09},
    author = {Sandhaus, Evan}

	file = {The New York Times Annotated Corpus - Linguistic Data Consortium:files/18/LDC2008T19.html:text/html}
    
}


@misc{github,
	title = {prateekpg2455/vewpoint},
	url = {https://github.com/prateekpg2455/vewpoint},
	abstract = {vewpoint - Capture variety of viewpoints in an article},
	urldate = {2014-12-09},
    howpublished = "\url{https://github.com/prateekpg2455/vewpoint}"
}

@article{lda_blei,
	title = {Latent Dirichlet Allocation},
	volume = {3},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=944919.944937},
	abstract = {We describe latent Dirichlet allocation ({LDA}), a generative probabilistic model for collections of discrete data such as text corpora. {LDA} is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an {EM} algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic {LSI} model.},
	urldate = {2014-12-09},
	journal = {J. Mach. Learn. Res.},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	month = mar,
	year = {2003},
	pages = {993--1022}
}

@article{diff_topic_model,
	title = {Differential Topic Models},
	volume = {99},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2014.2313127},
	number = {{PrePrints}},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ding, Nan and Chen, Changyou and Buntine, Wray and Du, Lan and Xie, Lexing},
	year = {2014},
	pages = {1},
	annote = {Complete {PDF} document was either not available or accessible. Please make sure you're logged in to the digital library to retrieve the complete {PDF} document.},
	file = {IEEE Computer Snapshot:files/21/06777293-abs.html:text/html}
}

@article{blei_prob_model_paper,
	title = {Probabilistic Topic Models},
	volume = {55},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/2133806.2133826},
	doi = {10.1145/2133806.2133826},
	abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
	number = {4},
	urldate = {2014-12-09},
	journal = {Commun. {ACM}},
	author = {Blei, David M.},
	month = apr,
	year = {2012},
	pages = {77--84},
	file = {ACM Full Text PDF:files/13/Blei - 2012 - Probabilistic Topic Models.pdf:application/pdf}
}


@incollection{supervised_topic_model,
	title = {Supervised Topic Models},
	url = {http://papers.nips.cc/paper/3328-supervised-topic-models.pdf},
	urldate = {2014-12-09},
	booktitle = {Advances in Neural Information Processing Systems 20},
	publisher = {Curran Associates, Inc.},
	author = {Mcauliffe, Jon D. and Blei, David M.},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	year = {2008},
	pages = {121--128},
	file = {NIPS Full Text PDF:files/15/Mcauliffe and Blei - 2008 - Supervised Topic Models.pdf:application/pdf;NIPS Snapshort:files/16/3328-supervised-topic-models.html:text/html}
}

@inproceedings{ptm,
	address = {Stroudsburg, {PA}, {USA}},
	series = {{EMNLP} '09},
	title = {Polylingual Topic Models},
	isbn = {978-1-932432-62-6},
	url = {http://dl.acm.org/citation.cfm?id=1699571.1699627},
	abstract = {Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts. Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model's characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.},
	urldate = {2014-12-09},
	booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Wallach, Hanna M. and Naradowsky, Jason and Smith, David A. and McCallum, Andrew},
	year = {2009},
	pages = {880--889},
	file = {ACM Full Text PDF:files/6/Mimno et al. - 2009 - Polylingual Topic Models.pdf:application/pdf}
}

@inproceedings{tm_lda,
	address = {New York, {NY}, {USA}},
	series = {{KDD} '12},
	title = {{TM}-{LDA}: Efficient Online Modeling of Latent Topic Transitions in Social Media},
	isbn = {978-1-4503-1462-6},
	shorttitle = {{TM}-{LDA}},
	url = {http://doi.acm.org/10.1145/2339530.2339552},
	doi = {10.1145/2339530.2339552},
	abstract = {Latent topic analysis has emerged as one of the most effective methods for classifying, clustering and retrieving textual data. However, existing models such as Latent Dirichlet Allocation ({LDA}) were developed for static corpora of relatively large documents. In contrast, much of the textual content on the web, and especially social media, is temporally sequenced, and comes in short fragments, including microblog posts on sites such as Twitter and Weibo, status updates on social networking sites such as Facebook and {LinkedIn}, or comments on content sharing sites such as {YouTube}. In this paper we propose a novel topic model, Temporal-{LDA} or {TM}-{LDA}, for efficiently mining text streams such as a sequence of posts from the same author, by modeling the topic transitions that naturally arise in these data. {TM}-{LDA} learns the transition parameters among topics by minimizing the prediction error on topic distribution in subsequent postings. After training, {TM}-{LDA} is thus able to accurately predict the expected topic distribution in future posts. To make these predictions more efficient for a realistic online setting, we develop an efficient updating algorithm to adjust the topic transition parameters, as new documents stream in. Our empirical results, over a corpus of over 30 million microblog posts, show that {TM}-{LDA} significantly outperforms state-of-the-art static {LDA} models for estimating the topic distribution of new documents over time. We also demonstrate that {TM}-{LDA} is able to highlight interesting variations of common topic transitions, such as the differences in the work-life rhythm of cities, and factors associated with area-specific problems and complaints.},
	urldate = {2014-12-13},
	booktitle = {Proceedings of the 18th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {{ACM}},
	author = {Wang, Yu and Agichtein, Eugene and Benzi, Michele},
	year = {2012},
	keywords = {mining social media data, temporal language models, topic transition modeling},
	pages = {123--131},
	file = {ACM Full Text PDF:/Users/mac/Library/Application Support/Firefox/Profiles/u7ckyl4g.default/zotero/storage/M47RVSH6/Wang et al. - 2012 - TM-LDA Efficient Online Modeling of Latent Topic .pdf:application/pdf}
}

